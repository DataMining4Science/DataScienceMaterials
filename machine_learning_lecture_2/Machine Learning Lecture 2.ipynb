{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Machine Learning Lecture 2"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Abstract:**  These exercises will familiarize with more advanced machine learning concepts including hyperparameter tuning and cross-validation.  You will also see how to visualize the model that is learned by a machine learning classifier.\n",
      "\n",
      "## Strategies for Preventing Overfitting: Regularization\n",
      "\n",
      "Last time we discussed machine learning we examined the tradeoff of the number of training examples and the performance of the model on predicting new instances.  We did this by producing graphs of model performance as a function of the amount of training data used:\n",
      "\n",
      "<div align=\"center\"><img src=\"files/images/digits_learning_curve_no_reg.png\"/></div>\n",
      "\n",
      "This graph shows the performance of a logistic regression model at predicting which of digit is represented by an 8 pixel by 8 pixel grayscale image of a hand written digit.\n",
      "\n",
      "While it is nice that the model\u2019s performance increases as we add more data, it would be nice if there was a way to have it all: better performance with less data.  Fortunately, many machine learning algorithms have strategies that attempt to achieve this goal.  Consider the objective function of the standard logistic regression model that we talked about a couple of lectures ago:\n",
      "\n",
      "<img src=\"files/images/logistic_equation.png\"/>\n",
      "\n",
      "We can augment this objective with a term that serves to penalize large weights (i.e. large values of the entries of w).  This modification serves to reduce the flexibility of fitting the training data, thereby improving the performance on predicting future data.  The modified objective function is:\n",
      "\n",
      "<img src=\"files/images/logistic_with_l2.png\"/>\n",
      "\n",
      "Where C is a positive constant that balance how much we care about fitting the training data compared to penalizing large weights.  To understand this a bit better, let\u2019s consider the limiting behavior.  How would this new version of logistic regression behave as C goes to 0?  How about as C goes to infinity?\n",
      "\n",
      "We can rerun our experiment on learning to recognize handwritten digits with this modified version of logistic regression.  For starters, let\u2019s just use the default value of C=1.  The learning curve now looks like this:\n",
      "\n",
      "<div align=\"center\"><img src=\"files/images/digits_learning_curve_C%3D1.png\"/></div>\n",
      "\n",
      "Woah!  This is a lot better\u2026 However, we might ask ourselves whether we can do even better if we tuned the value of C a little bit.\n",
      "\n",
      "## Tuning Hyper Parameters\n",
      "\n",
      "It turns out that properly tuning the values of constants such as C (the penalty for large weights in the logistic regression model) is perhaps the most important skill for successfully applying machine learning to a problem.  Let\u2019s see how this learning curve will look with different values of C:\n",
      "\n",
      "<div align=\"center\"><img src=\"files/images/digits_learning_curve_C_comparison.png\"/></div>\n",
      "\n",
      "If we zoom in a bit on the more interesting part of the graph:\n",
      "\n",
      "<div align=\"center\"><img src=\"files/images/digits_learning_curve_C_comparison_zoomed.png\"/></div>\n",
      "\n",
      "It looks like we can do a bit better than the default value of C=1 by choosing C=0.01.  How well do would we expect our model to do on predicting images of handwritten digits if we were to collect a brand new database?\n",
      "\n",
      "Luckily, Scikit-learn provides some built-in mechanisms for doing parameter tuning in a sensible manner.  One such method is to use a cross validation to choose the optimal setting of a particular parameter.\n",
      "\n",
      "Cross validation can be performed in scikit-learn using the following code:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.datasets import *\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "data = load_digits()\n",
      "\n",
      "tuned_parameters = [{'C': [10**-4, 10**-2, 10**0, 10**2, 10**4]}]\n",
      "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, train_size=.9)\n",
      "\n",
      "model = GridSearchCV(LogisticRegression(), tuned_parameters, cv=5)\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "print model.best_estimator_\n",
      "print model.score(X_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Visualizing the Learned Model\n",
      "\n",
      "In some cases it may be informative to examine the pattern of weights learned by a machine learning model.  This visualization can either be useful for understanding your data in a new way, or as a method of creating new features in order to improve model performance.\n",
      "\n",
      "To visualize how the logistic regression model discriminates between the various classes of digits, we use the following code:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "from sklearn.datasets import *\n",
      "import matplotlib.pyplot as plt\n",
      "import numpy\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "\n",
      "data = load_digits()\n",
      "model = LogisticRegression()\n",
      "model.fit(data.data, data.target)\n",
      "\n",
      "fig = plt.figure()\n",
      "\n",
      "for i in range(10):\n",
      "    subplot = fig.add_subplot(5,2,i+1)\n",
      "    subplot.matshow( numpy.reshape(model.raw_coef_[i][1:], (8,8)), cmap='gray')\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Non-linear Models\n",
      "\n",
      "Now that we have learned the basics of the process of applying a particular machine learning algorithm (logistic regression), we can start to explore more advanced machine learning algorithms.  There are probably hundreds of machine learning algorithms for every step of the machine learning pipeline.  Scikit learn has some of the built-in (check out the documentation [here](http://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html)).  Next, I will briefly show how we can apply a new machine learning algorithm to the task of classifying digits.\n",
      "\n",
      "A very powerful machine learning algorithm is the [support vector machine](http://en.wikipedia.org/wiki/Support_vector_machine) (or SVM).  Part of the reason that this algorithm is so powerful is that it is capable of learning non-linear decision boundaries.\n",
      "\n",
      "In order to apply the support vector machine to the digit classification problem, we need to intelligently tune the parameters of the algorithm (or else we will get suboptimal performance).  In contrast the logistic regression model, the support vector machine has more hyper parameters to tune.  Here is a snippet of code that tunes the parameters of a support vector machine:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.grid_search import GridSearchCV\n",
      "from sklearn.datasets import *\n",
      "from sklearn.linear_model import LogisticRegression\n",
      "from sklearn.svm import SVC\n",
      "\n",
      "data = load_digits()\n",
      "\n",
      "tuned_parameters = [{'kernel': ['rbf'],'gamma': [1e-3, 1e-4], 'C': [1, 10, 100, 1000]},\n",
      "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
      "\n",
      "\n",
      "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, train_size=0.9)\n",
      "model = GridSearchCV(SVC(C=1), tuned_parameters, cv=5)\n",
      "\n",
      "model.fit(X_train, y_train)\n",
      "\n",
      "print model.best_estimator_\n",
      "print model.score(X_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Experiment with other built-in scikit-learn classifiers.  For instance, can you use a similar methodology as above but instead apply [k-nearest neighbors](http://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html#k-nearest-neighbors-classifier) or [random forests](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier)?  Try this out in the cell below.  Which method appears to do the best?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}